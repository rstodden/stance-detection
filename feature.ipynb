{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbf3e45-6dd6-4567-834e-19e80cd2149e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HDD2TB/conda_envs/argmining-pattern/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "from transformers import pipeline\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b458b3a-90c9-41b2-b2a7-1f4cb82a4636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "claim_connective_premise = [\"conversely\", \"whereas\", \"but\", \"however\", \"while\", \"although\", \"though\", \"meanwhile\", \n",
    "                            \"previously\", \"instead\", \"or\", \"and\", \"then\", \"when\", \"as\", \"for\", \"accordingly\", \n",
    "                            \"because\", \"with\", \"since\", \"as\", \"and\", \"when\", \"if\", \"but\"]\n",
    "premise_connective_claim = [\"yet\", \"still\", \"nevertheless\", \"rather\", \"nonetheless\", \"therefore\", \"thereby\", \"so\",\n",
    "                            \"hence\", \"consequently\", \"thus\"]\n",
    "unclear_order = ['additionally', 'after', 'afterward', 'also', 'alternatively', 'anyway', 'before', 'despite', 'else', \n",
    "                 'essentially', 'eventually', 'except', 'finally', 'further', 'furthermore', 'indeed', 'later', 'lest', \n",
    "                 'likewise', 'meantime', 'moreover', 'next', 'once', 'otherwise', 'overall', 'particularly', 'plus',\n",
    "                 'regardless', 'separately', 'similarly', 'simultaneously', 'specifically', 'thereafter', 'till', \n",
    "                 'ultimately', 'unless', 'until', 'upon', 'whatever', 'whenever', 'without', 'earlier', 'nor'] # claim premise as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0707afa-b9d6-4a1a-884d-28a0b3d3138c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': {'attack': ['although',\n",
       "   'though',\n",
       "   'but',\n",
       "   'nevertheless',\n",
       "   'if',\n",
       "   'previously',\n",
       "   'when',\n",
       "   'and',\n",
       "   'then',\n",
       "   'while',\n",
       "   'however',\n",
       "   'still',\n",
       "   'or',\n",
       "   'instead',\n",
       "   'as',\n",
       "   'nonetheless',\n",
       "   'meanwhile',\n",
       "   'yet',\n",
       "   'rather',\n",
       "   'besides',\n",
       "   'nor',\n",
       "   'whereas',\n",
       "   'earlier',\n",
       "   'conversely'],\n",
       "  'support': ['but',\n",
       "   'because',\n",
       "   'if',\n",
       "   'when',\n",
       "   'and',\n",
       "   'so',\n",
       "   'then',\n",
       "   'as',\n",
       "   'since',\n",
       "   'therefore',\n",
       "   'thus',\n",
       "   'thereby',\n",
       "   'consequently',\n",
       "   'hence',\n",
       "   'accordingly',\n",
       "   'for',\n",
       "   'with',\n",
       "   'given'],\n",
       "  'both': ['but', 'if', 'when', 'and', 'then', 'as'],\n",
       "  'all': ['once',\n",
       "   'although',\n",
       "   'though',\n",
       "   'but',\n",
       "   'because',\n",
       "   'nevertheless',\n",
       "   'before',\n",
       "   'until',\n",
       "   'if',\n",
       "   'previously',\n",
       "   'when',\n",
       "   'and',\n",
       "   'so',\n",
       "   'then',\n",
       "   'while',\n",
       "   'however',\n",
       "   'also',\n",
       "   'after',\n",
       "   'separately',\n",
       "   'still',\n",
       "   'or',\n",
       "   'moreover',\n",
       "   'instead',\n",
       "   'as',\n",
       "   'nonetheless',\n",
       "   'unless',\n",
       "   'meanwhile',\n",
       "   'yet',\n",
       "   'since',\n",
       "   'rather',\n",
       "   'indeed',\n",
       "   'later',\n",
       "   'ultimately',\n",
       "   'therefore',\n",
       "   'thus',\n",
       "   'further',\n",
       "   'afterward',\n",
       "   'next',\n",
       "   'similarly',\n",
       "   'besides',\n",
       "   'nor',\n",
       "   'alternatively',\n",
       "   'whereas',\n",
       "   'overall',\n",
       "   'till',\n",
       "   'finally',\n",
       "   'otherwise',\n",
       "   'thereby',\n",
       "   'additionally',\n",
       "   'meantime',\n",
       "   'likewise',\n",
       "   'regardless',\n",
       "   'thereafter',\n",
       "   'earlier',\n",
       "   'except',\n",
       "   'furthermore',\n",
       "   'lest',\n",
       "   'specifically',\n",
       "   'conversely',\n",
       "   'consequently',\n",
       "   'plus',\n",
       "   'hence',\n",
       "   'accordingly',\n",
       "   'simultaneously',\n",
       "   'for',\n",
       "   'else',\n",
       "   'whatever',\n",
       "   'when/then',\n",
       "   'everytime',\n",
       "   'despite',\n",
       "   'without',\n",
       "   'with',\n",
       "   'essentially',\n",
       "   'given',\n",
       "   'anyway',\n",
       "   'upon',\n",
       "   'eventually',\n",
       "   'whenever',\n",
       "   'particularly']},\n",
       " '20': {'attack': ['although',\n",
       "   'though',\n",
       "   'but',\n",
       "   'nevertheless',\n",
       "   'while',\n",
       "   'however',\n",
       "   'still',\n",
       "   'yet',\n",
       "   'rather',\n",
       "   'whereas',\n",
       "   'conversely'],\n",
       "  'support': ['because',\n",
       "   'so',\n",
       "   'as',\n",
       "   'since',\n",
       "   'therefore',\n",
       "   'thus',\n",
       "   'thereby',\n",
       "   'consequently',\n",
       "   'hence',\n",
       "   'accordingly',\n",
       "   'for',\n",
       "   'with',\n",
       "   'given']},\n",
       " '40': {'attack': ['although',\n",
       "   'though',\n",
       "   'but',\n",
       "   'nevertheless',\n",
       "   'while',\n",
       "   'however',\n",
       "   'still',\n",
       "   'yet',\n",
       "   'whereas',\n",
       "   'conversely'],\n",
       "  'support': ['because',\n",
       "   'so',\n",
       "   'since',\n",
       "   'therefore',\n",
       "   'thus',\n",
       "   'thereby',\n",
       "   'consequently',\n",
       "   'hence',\n",
       "   'accordingly',\n",
       "   'for',\n",
       "   'with']},\n",
       " '80': {'attack': ['but', 'however', 'whereas', 'conversely'],\n",
       "  'support': ['because',\n",
       "   'so',\n",
       "   'therefore',\n",
       "   'thus',\n",
       "   'thereby',\n",
       "   'consequently',\n",
       "   'hence',\n",
       "   'accordingly',\n",
       "   'for']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"connectives_dict.json\") as f:\n",
    "    connective_dict = json.load(f)\n",
    "connective_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5224c7-080f-466c-9bf6-80696467c609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"connectives_dict_reduced_BERT.json\") as f:\n",
    "    connective_dict_bert = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6486df70-4a15-4597-8a81-24aba5b82ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"connectives_dict_reduced-RoBERTa.json\") as f:\n",
    "    connective_dict_roberta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb42521e-5237-4cf9-86f5-82e98d7502fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"connectives_dict_reduced_XLM-RoBERTa.json\") as f:\n",
    "    connective_dict_clmroberta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16499851-0332-4528-aa89-9337ea1e1c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb215c5e-bb3d-47d5-888e-0170ca389e41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accordingly', 'consequently', 'hence', 'thereby', 'therefore', 'thus'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(connective_dict_bert[\"all\"][\"support\"]).symmetric_difference(set(connective_dict_roberta[\"all\"][\"support\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58f4558f-c03a-4a64-bf62-f0ea609c0001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_new_sent(text1, text2, pattern):\n",
    "    if text1.strip()[-1] in [\".\", \",\", \"!\", \"?\", \":\", \";\", \"-\"]:\n",
    "        text1 = text1[:-1]\n",
    "    # print(text1)\n",
    "    if text2.strip().capitalize() == text2.strip():\n",
    "        text2 = text2[0].strip().lower()+text2[1:].strip()  \n",
    "        # todo remove strip here otherwise \"a\" at sentence beginning will be concatenated with next word\n",
    "    # print(text2)\n",
    "    # print(\" \".join([text1.strip(), pattern, text2.strip().lower()]).strip())\n",
    "    return \" \".join([text1.strip(), pattern, text2.strip().lower()]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01ff6a-b656-43aa-b404-5debd177f83a",
   "metadata": {},
   "source": [
    "- removed in bert:\n",
    "    [\"moreover\", \"furthermore\", \"lest\", \"conversely\", \"when/then\", \"everytime\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef2a41-a5c6-4d4f-ab7e-515ee8b16419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmin_all all_conns_bert all_conns distilbert-base-uncased\n",
      "1000 11139\n",
      "8000 11139\n",
      "10000 11139\n",
      "12000 11139\n",
      "14000 11139\n",
      "21000 11139\n",
      "24000 11139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmin_all all_conns_bert all_conns bert-base-uncased\n",
      "0 11139\n",
      "1000 11139\n",
      "2000 11139\n"
     ]
    }
   ],
   "source": [
    "# connective_dict = connective_dict_bert\n",
    "# connective_name = \"all_conns_bert\"\n",
    "# model_list = [\"distilbert-base-uncased\", \"bert-base-uncased\", \"bert-large-uncased\"]\n",
    "\n",
    "for connective_dict, connective_name, model_list in [(connective_dict_roberta, \"all_conns_roberta\", [\"distilbert-base-uncased\", \"bert-base-uncased\", \"bert-large-uncased\", \"roberta-base\", \"xlm-roberta-base\", \"roberta-large\", \"xlm-roberta-large\"])]:\n",
    "    # (connective_dict_bert, \"all_conns_bert\", [\"distilbert-base-uncased\", \"bert-base-uncased\", \"bert-large-uncased\"]),\n",
    "   #  (connective_dict_xlm-roberta, \"all_conns_xlm-roberta\", [\"distilbert-base-uncased\", \"bert-base-uncased\", \"bert-large-uncased\", \"xlm-roberta-base\", \"xlm-roberta-large\"])\n",
    "    name = \"all_conns\"\n",
    "    split_perc = \"all\"\n",
    "    split_conn = \"all\"\n",
    "    # for name, split_perc, split_conn in [(\"all_conns\", \"all\", \"all\")]: # , (\"all_attack_support\", \"all\", \"attack+support\"),  (\"all_attack\", \"all\", \"attack\"),  (\"all_support\", \"all\", \"support\"),  (\"all_both\", \"all\", \"both\")]:\n",
    "    for data_path in [\"argmin_all.csv\", \"ibmcs_all.csv\", \"perspectrum_all.csv\"]:\n",
    "        data_name = data_path[:-4]\n",
    "        df = pd.read_csv(\"data/\"+data_path, index_col=0)\n",
    "        output_data = []\n",
    "        if split_conn == \"attack+support\":\n",
    "            connectives = connective_dict[split_perc][\"attack\"] + connective_dict[split_perc][\"support\"]\n",
    "        else:\n",
    "            connectives = connective_dict[split_perc][split_conn]\n",
    "        for model_name in model_list:  # , \"roberta-base\", \"xlm-roberta-base\"]:  # [\"microsoft/deberta-base\"]:  # [\"distilbert-base-cased\", \"bert-base-cased\", \"roberta-base\", \"xlm-roberta-base\", \"bert-large-cased\", \"roberta-large\", \"xlm-roberta-large\"]:\n",
    "            classifier = pipeline(\"fill-mask\", model=model_name, device=0)\n",
    "            masked_token = classifier.tokenizer.mask_token\n",
    "            print(data_name, connective_name, name, model_name)\n",
    "            for i, row in df.iterrows():\n",
    "                output_dict = row.to_dict()\n",
    "                new_sent_clwp = get_new_sent(row[\"claim\"], row[\"premise\"], masked_token)\n",
    "                new_sent_plwc = get_new_sent(row[\"premise\"], row[\"claim\"], masked_token)\n",
    "                output_dict[\"input_clwp\"] = new_sent_clwp\n",
    "                output_dict[\"input_plwc\"] = new_sent_plwc\n",
    "                result_clwp = classifier(new_sent_clwp, targets=connectives, top_k=classifier.model.config.vocab_size)\n",
    "                result_plwc = classifier(new_sent_plwc, targets=connectives, top_k=classifier.model.config.vocab_size)\n",
    "                # connectives = set([item[\"token_str\"] for item in result])\n",
    "                # print(model_name, sorted(list(set(connective_dict[split_conn][split_perc]).difference(connectives))))\n",
    "                for item in result_clwp:\n",
    "                    if item[\"token_str\"] in connectives and (item[\"token_str\"] in claim_connective_premise or item[\"token_str\"] in unclear_order):\n",
    "                        # df.loc[i, model_name+\"_\"+item[\"token_str\"]] =  item[\"score\"]\n",
    "                        output_dict[model_name+\"_\"+item[\"token_str\"]] =  item[\"score\"]\n",
    "                for item in result_plwc:\n",
    "                    if item[\"token_str\"] in connectives and item[\"token_str\"] in premise_connective_claim:\n",
    "                        # df.loc[i, model_name+\"_\"+item[\"token_str\"]] =  item[\"score\"]\n",
    "                        output_dict[model_name+\"_\"+item[\"token_str\"]] =  item[\"score\"]\n",
    "                if i%1000 == 0:\n",
    "                    print(i, len(df))\n",
    "                output_data.append(output_dict)\n",
    "            df = pd.DataFrame(output_data)\n",
    "            output_data = []\n",
    "        if not os.path.exists(\"features/\"+connective_name+\"/\"+name):\n",
    "            os.makedirs(\"features/\"+connective_name+\"/\"+name)\n",
    "        df.to_csv(\"features/\"+connective_name+\"/\"+name+\"/\"+data_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e77b33-4b0d-4767-9e97-5ef37f9cc326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
